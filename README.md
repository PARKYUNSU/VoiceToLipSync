# VoiceToLipSync: High-Quality Lip Sync Video Generation

PyTorch implementation of LipSync-Voice: High-Quality Lip Sync Video Generation using Wav2Lip models.

### Tech Stack

<p align="left">

  <img src="https://img.shields.io/badge/Library-PyTorch-lightgrey" alt="PyTorch"/>
  <img src="https://img.shields.io/badge/Library-OpenCV-black" alt="OpenCV"/>
  <img src="https://img.shields.io/badge/Library-Librosa-orange" alt="Librosa"/>
  <img src="https://img.shields.io/badge/Model-GAN-lightgrey" alt="GAN"/>
  <img src="https://img.shields.io/badge/Library-Wav2Lip-blue" alt="Wav2Lip"/>
  <img src="https://img.shields.io/badge/Library-OpenVoice-teal" alt="OpenVoice"/>
</p>

---
LipSync-Voice is a high-quality lip-sync video generation system that leverages deep learning to synchronize facial movements with speech. This framework processes input audio and video through ``LipSync.ipynb``, extracting speech features and synchronizing them with facial motions to create realistic lip-sync videosâ€”all without requiring manual installation of dependencies.

## Project Duration

**2025.01.01 - 2025.01.20**

<table>
  <tbody>
    <tr>
      <td align="center">
        <a href="https://github.com/PARKYUNSU">
          <img src="https://github.com/PARKYUNSU.png" width="100px;" alt=""/>
          <br /><sub><b>Yunsu Park</b></sub>
        </a>
        <br />
      <td align="center">
        <a href="https://github.com/MyoungJinSon">
          <img src="https://github.com/MyoungJinSon.png" width="100px;" alt=""/>
          <br /><sub><b>Myoungjin Son</b></sub>
        </a>
        <br />
      </td>
    </tr>
  </tbody>
</table>

## Presentation

The presentation deck is available in the `deck` folder: [LipSync_Voice_Presentation.pdf](https://github.com/PARKYUNSU/LipSync-Voice/blob/main/deck/LipSync_Voice_Presentation.pdf).

## How to Use

1. Click the "Open in Colab" button below.
2. Follow the instructions in the notebook to upload your video files.
3. Generate the lip-sync video and download the result.

## VoiceToLipSync Demo

Click the badge below to run the demo

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PARKYUNSU/VoiceToLipSync/blob/main/Voice2LipSync.ipynb)

## Results
### Reference Video
https://github.com/user-attachments/assets/8e48cdc3-eb0b-4e3c-9cb2-222f8edfd3cd

### Lip Synced Video
https://github.com/user-attachments/assets/ffc0ac9f-9944-4ff9-aa19-3139182f5249

## References

### Lip Syncing
- **Wav2Lip**: A model for lip-syncing in videos, where the facial movements are synchronized with the audio.  
  Repository: [https://github.com/Rudrabha/Wav2Lip](https://github.com/Rudrabha/Wav2Lip)

### Speech Synthesis and Voice Conversion
- **OpenVoice**: A deep learning framework for text-to-speech conversion with high-quality voice synthesis.  
  Repository: [https://github.com/OpenVoice](https://github.com/OpenVoice)

---
